{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "https://iamtrask.github.io/2015/07/27/python-network-part2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy (mathematics library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as $\\frac{1}{1+e^{-x}}$. The derivative of that is $s(x) * (1 - s(x))$ where $s(x)$ is the sigmoid function. This function definition doesn't define itself recursively because in actual usage the computed value of the original sigmoid will be passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs. Each row is a data point that will be used to train the network.\n",
    "\n",
    "`numpy.array` - generates a vector if passed in a list or a matrix if passed in a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.array().T` - if passed in a matrix, it gets transposed, otherwise it remains the same\n",
    "\n",
    "The expected outputs. This is equivalent to\n",
    "```\n",
    "np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1]\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [0, 0, 1, 1]\n",
    "]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the random number generator with a definitive number so that we can see progress each time it is run.\n",
    "\n",
    "PRNG (Pseudo Random Number Generators) are very \"random\" given that the initial value, the seed, is different each time. If the seed is the same number, then it will be deterministic. For example, if the seed function is passed 1 I can expect x, y, and z when the random function is run three times. If I seed it with 1 again, I can be certain that when run three times I will get x, y, and z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the first layer's weights randomly with a mean of 0.\n",
    "\n",
    "This has a mean of 0 as $E(x)$ is defined as the sum of all possible values multiplied by its probability. Since `np.random.random` yields values between 0 and 1 and all values between them should be equally probable we end up with 0.5 as the expected value. Multiplying that by 2 and subtracting 1 yields $E(x) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2 * np.random.random((3, 1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training takes place in the for loop.\n",
    "\n",
    "The first layer (`layer0`) is just the inputs, in this case `x`.\n",
    "\n",
    "The second layer (`layer1`) is derived doing a matrix multiplication of `layer0` and `syn0` aka the inputs and the weights. `layer0` is a 4x4 matrix and `syn0` is a 3x1 matrix, so it's just a vector matrix multiplication. In this case it'd be equivalent to:\n",
    "\n",
    "```\n",
    "[\n",
    "    [ syn0[0][0]*layer0[0][0] + syn[1][0]*layer[0][1] + syn[2][0]*layer[0][2] ],\n",
    "    [ syn0[0][0]*layer0[1][0] + syn[1][0]*layer[1][1] + syn[2][0]*layer[1][2] ],\n",
    "    [ syn0[0][0]*layer0[2][0] + syn[1][0]*layer[2][1] + syn[2][0]*layer[2][2] ],\n",
    "    [ syn0[0][0]*layer0[3][0] + syn[1][0]*layer[3][1] + syn[2][0]*layer[3][2] ],\n",
    "]\n",
    "```\n",
    "\n",
    "This is fed into the the sigmoid function to normalize it between 0 and 1. That is taken and `layer1_error` is calculated by seeing the difference between the expected value and the value output from the sigmoid function.\n",
    "\n",
    "Next the error is multiplied by the slope of the sigmoid function at the values in `layer1`. This is then taken and a matrix multiplication is done between the inputs transposed and the delta derived from multiplying the error and the slopes.\n",
    "\n",
    "The intuition about the delta is that in a sigmoid function, the slope near -1 and 1 is very low. This will not change the error by much but if the outputs are uncertain, meaning that it's near 0 the slope will be big and change the weights by a lot more. The delta is then added to the weights at which point we can rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training: \n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    # layer 0 aka the inputs\n",
    "    layer0 = x\n",
    "\n",
    "    # layer 1\n",
    "    layer1 = sigmoid(np.dot(layer0, syn0))\n",
    "\n",
    "    layer1_error = y - layer1\n",
    "    layer1_delta = layer1_error * sigmoid(layer1, deriv=True)\n",
    "\n",
    "    # adjust the weights\n",
    "    syn0 += np.dot(layer0.T, layer1_delta)\n",
    "    \n",
    "print('Output after training: ')\n",
    "print(layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation does not optimize. It just moves all the error information from the end of the network to all the weights inside the network. Another algorithm has to then work with the information generated.\n",
    "\n",
    "One such algorithm is gradient descent. Simplified gradient descent can be seen as such:\n",
    "\n",
    "1. Calculate slope at current position\n",
    "2. If slope is negative, move right\n",
    "3. If slope is positive, move left\n",
    "4. Repeat above steps until slope is 0\n",
    "\n",
    "Naive gradient descent:\n",
    "\n",
    "1. Calculate slope at current x position\n",
    "2. Change x by the negative of the slope ($x = x - slope$)\n",
    "3. Repeat above steps until slope is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1: Overshooting\n",
    "\n",
    "Overshooting can lead to divergence. To resolve this the slope can be simply made smaller. The gradients are multiplied by a number between 0 and 1 called the alpha.\n",
    "\n",
    "Improved gradient descent:\n",
    "1. Set alpha to some number between 0 and 1\n",
    "2. Calculate slope at current x position\n",
    "3. $x = x - (alpha * slope)$\n",
    "4. Repeat above steps until slope is 0\n",
    "\n",
    "#### Problem 2: Local Minimums\n",
    "\n",
    "The algorithm might get caught at a local minimum, where it satisfies the condition that the slope is 0 and it will quit.\n",
    "\n",
    "To resolve this, we can have multiple random starting points. Neural networks do this by having very large hidden layers. Each hidden node in a layer starts out in a different random starting state. This allows  the nodes to converge to different patterns.\n",
    "\n",
    "#### Problem 3: Slopes are Too Small\n",
    "\n",
    "This can occur if the alpha is too small and can be remedied by increasing the alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [0, 0, 1, 1]\n",
    "]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed it with a deteministic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights with a mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synapse0 = 2 * np.random.random((2, 1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training is done below.\n",
    "\n",
    "`layer0` is just the training data set and `layer1` is just the data and the weights multiplied and normalized against the activation function.\n",
    "\n",
    "`synapse0_derivative` is obtained by multiplying the delta and the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training:\n",
      "[[ 0.00505119]\n",
      " [ 0.00505119]\n",
      " [ 0.99494905]\n",
      " [ 0.99494905]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    layer0 = x\n",
    "    layer1 = sigmoid(np.dot(layer0, synapse0))\n",
    "    \n",
    "    layer1_error = layer1 - y\n",
    "    layer1_delta = layer1_error * sigmoid(layer1, deriv=True)\n",
    "\n",
    "    synapse0_derivative = np.dot(layer0.T, layer1_delta)\n",
    "    synapse0 -= synapse0_derivative\n",
    "\n",
    "print('Output after training:')\n",
    "print(layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha parameter reduces the size of each iteration's update. Right before we update the weights, we multiply the weight update by the alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the inputs and the expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([[0, 1, 1, 0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is training with different alphas to show the effect of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with alpha: 0.001\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.495164025493\n",
      "Error after 20000 iterations: 0.493596043188\n",
      "Error after 30000 iterations: 0.491606358559\n",
      "Error after 40000 iterations: 0.489100166544\n",
      "Error after 50000 iterations: 0.485977857846\n",
      "\n",
      "Training with alpha: 0.01\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.457431074442\n",
      "Error after 20000 iterations: 0.359097202563\n",
      "Error after 30000 iterations: 0.239358137159\n",
      "Error after 40000 iterations: 0.143070659013\n",
      "Error after 50000 iterations: 0.0985964298089\n",
      "\n",
      "Training with alpha: 0.1\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.0428880170001\n",
      "Error after 20000 iterations: 0.0240989942285\n",
      "Error after 30000 iterations: 0.0181106521468\n",
      "Error after 40000 iterations: 0.0149876162722\n",
      "Error after 50000 iterations: 0.0130144905381\n",
      "\n",
      "Training with alpha: 1\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.00858452565325\n",
      "Error after 20000 iterations: 0.00578945986251\n",
      "Error after 30000 iterations: 0.00462917677677\n",
      "Error after 40000 iterations: 0.00395876528027\n",
      "Error after 50000 iterations: 0.00351012256786\n",
      "\n",
      "Training with alpha: 10\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.00312938876301\n",
      "Error after 20000 iterations: 0.00214459557985\n",
      "Error after 30000 iterations: 0.00172397549956\n",
      "Error after 40000 iterations: 0.00147821451229\n",
      "Error after 50000 iterations: 0.00131274062834\n",
      "\n",
      "Training with alpha: 100\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.125476983854\n",
      "Error after 20000 iterations: 0.125330333529\n",
      "Error after 30000 iterations: 0.125267728767\n",
      "Error after 40000 iterations: 0.125231073663\n",
      "Error after 50000 iterations: 0.125206352759\n",
      "\n",
      "Training with alpha: 1000\n",
      "Error after 0 iterations: 0.496410031903\n",
      "Error after 10000 iterations: 0.5\n",
      "Error after 20000 iterations: 0.5\n",
      "Error after 30000 iterations: 0.5\n",
      "Error after 40000 iterations: 0.5\n",
      "Error after 50000 iterations: 0.5\n"
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "    print('\\nTraining with alpha: {}'.format(str(alpha)))\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    synapse0 = 2 * np.random.random((3, 4)) - 1\n",
    "    synapse1 = 2 * np.random.random((4, 1)) - 1\n",
    "    \n",
    "    for j in range(60000):\n",
    "        layer0 = x\n",
    "        layer1 = sigmoid(np.dot(layer0, synapse0))\n",
    "        layer2 = sigmoid(np.dot(layer1, synapse1))\n",
    "        \n",
    "        layer2_error = layer2 - y\n",
    "        layer2_delta = layer2_error * sigmoid(layer2, deriv=True)\n",
    "        \n",
    "        if j % 10000 == 0:\n",
    "            print('Error after {} iterations: {}'.format(str(j), str(np.mean(np.abs(layer2_error)))))\n",
    "        \n",
    "        layer1_error = layer2_delta.dot(synapse1.T)\n",
    "        layer1_delta = layer1_error * sigmoid(layer1, deriv=True)\n",
    "        \n",
    "        synapse1 -= alpha * layer1.T.dot(layer2_delta)\n",
    "        synapse0 -= alpha * layer0.T.dot(layer1_delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
