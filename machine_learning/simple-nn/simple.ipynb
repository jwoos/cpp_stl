{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "https://iamtrask.github.io/2015/07/12/basic-python-network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numpy (mathematics library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as $\\frac{1}{1+e^{-x}}$. The derivative of that is $s(x) * (1 - s(x))$ where $s(x)$ is the sigmoid function. This function definition doesn't define itself recursively because in actual usage the computed value of the original sigmoid will be passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs. Each row is a data point that will be used to train the network.\n",
    "\n",
    "`numpy.array` - generates a vector if passed in a list or a matrix if passed in a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.array().T` - if passed in a matrix, it gets transposed, otherwise it remains the same\n",
    "\n",
    "The expected outputs. This is equivalent to\n",
    "```\n",
    "np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1]\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [0, 0, 1, 1]\n",
    "]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the random number generator with a definitive number so that we can see progress each time it is run.\n",
    "\n",
    "PRNG (Pseudo Random Number Generators) are very \"random\" given that the initial value, the seed, is different each time. If the seed is the same number, then it will be deterministic. For example, if the seed function is passed 1 I can expect x, y, and z when the random function is run three times. If I seed it with 1 again, I can be certain that when run three times I will get x, y, and z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the first layer's weights randomly with a mean of 0.\n",
    "\n",
    "This has a mean of 0 as $E(x)$ is defined as the sum of all possible values multiplied by its probability. Since `np.random.random` yields values between 0 and 1 and all values between them should be equally probable we end up with 0.5 as the expected value. Multiplying that by 2 and subtracting 1 yields $E(x) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2 * np.random.random((3, 1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training takes place in the for loop.\n",
    "\n",
    "The first layer (`layer0`) is just the inputs, in this case `x`.\n",
    "\n",
    "The second layer (`layer1`) is derived doing a matrix multiplication of `layer0` and `syn0` aka the inputs and the weights. `layer0` is a 4x4 matrix and `syn0` is a 3x1 matrix, so it's just a vector matrix multiplication. In this case it'd be equivalent to:\n",
    "\n",
    "```\n",
    "[\n",
    "    [ syn0[0][0]*layer0[0][0] + syn[1][0]*layer[0][1] + syn[2][0]*layer[0][2] ],\n",
    "    [ syn0[0][0]*layer0[1][0] + syn[1][0]*layer[1][1] + syn[2][0]*layer[1][2] ],\n",
    "    [ syn0[0][0]*layer0[2][0] + syn[1][0]*layer[2][1] + syn[2][0]*layer[2][2] ],\n",
    "    [ syn0[0][0]*layer0[3][0] + syn[1][0]*layer[3][1] + syn[2][0]*layer[3][2] ],\n",
    "]\n",
    "```\n",
    "\n",
    "This is fed into the the sigmoid function to normalize it between 0 and 1. That is taken and `layer1_error` is calculated by seeing the difference between the expected value and the value output from the sigmoid function.\n",
    "\n",
    "Next the error is multiplied by the slope of the sigmoid function at the values in `layer1`. This is then taken and a matrix multiplication is done between the inputs transposed and the delta derived from multiplying the error and the slopes.\n",
    "\n",
    "The intuition about the delta is that in a sigmoid function, the slope near 0 and 1 is very low.\n",
    "\n",
    "This is then added to the weights at which point we can rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training: \n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    # layer 0 aka the inputs\n",
    "    layer0 = x\n",
    "\n",
    "    # layer 1\n",
    "    layer1 = sigmoid(np.dot(layer0, syn0))\n",
    "\n",
    "    layer1_error = y - layer1\n",
    "    layer1_delta = layer1_error * sigmoid(layer1, deriv=True)\n",
    "\n",
    "    # adjust the weights\n",
    "    syn0 += np.dot(layer0.T, layer1_delta)\n",
    "    \n",
    "print('Output after training: ')\n",
    "print(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
